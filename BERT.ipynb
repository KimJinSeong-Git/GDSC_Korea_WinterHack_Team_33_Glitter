{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GDSC_WinterHack.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bDINikzBtUmu"
      },
      "outputs": [],
      "source": [
        "!pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'\n",
        "!pip install torch==1.9.0 sentencepiece==0.1.96 transformers==4.8.1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 모듈 임포트"
      ],
      "metadata": {
        "id": "mopo6yh4vAeQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertModel\n",
        "from kobert_tokenizer import KoBERTTokenizer"
      ],
      "metadata": {
        "id": "QBbNxhC2th6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 모델 불러오기"
      ],
      "metadata": {
        "id": "xriDXpHLvCEC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertModel.from_pretrained('skt/kobert-base-v1')\n",
        "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')"
      ],
      "metadata": {
        "id": "M-PyInAFtlb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 문장 토큰화"
      ],
      "metadata": {
        "id": "9WPwA88ovDde"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence1 = \"민통선 바다를 건너왔다고 하여 산과 바다가 다르지는 않다고 생각했으나 그건 편견이었다. 월선포엔 태풍의 눈 같은 바다가 있었다. 헤라클라스의 풀무질 같은 거대한 힘의 물결이 소용돌이 치고 있었다. 바다가 아닌 폭우가 내린 다음날의 강줄기 같았다. 바다 건너엔 석모도 상주산이 솟구쳐 있었으나 살아 꿈틀거리고 있는 힘의 물살이 시선까지 빨아 당기고 있었다.\"\n",
        "sentence2 = \"햇살이 흐르는 수면에 찬란히 빛나는 것이, 거대한 은갈치가 휘감아 흐르는 듯했다. 어디서도 본 적 없는 바다가 월선포에 있었다. 교동대교가 생기기 전엔 육지와 섬을 잇는 유일한 통로였으나 지금은 빛바랜 ‘선착장 대합실’ 간판만 분주했던 옛 시절을 추억한다.\"\n",
        "\n",
        "token_ids1 = tokenizer.encode(sentence1)\n",
        "token_ids2 = tokenizer.encode(sentence2)"
      ],
      "metadata": {
        "id": "YhVkw99_tmLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_ids1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7hl149VyVEY",
        "outputId": "e4b92ccd-7a0a-4448-96d6-a03dd5c54a35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[   2, 2169, 7636, 6559, 2191, 6116,  881, 5691, 6990, 5439,  517,\n",
              "          7815, 2640, 5468, 2191, 5330, 1562, 6113, 7327, 3153, 5439, 2705,\n",
              "          7876, 1185, 5384, 4832, 5414, 7112,   54,  517, 7028, 6559, 7728,\n",
              "          6909, 4720, 7750, 7095, 1535,  833, 2191, 5330, 3873,   54, 5046,\n",
              "          6003, 7568, 6003, 6676, 4888, 6228, 7350,  833,  862, 5827, 5211,\n",
              "          7095, 2135, 5415, 7096, 2822, 7003, 5868, 7096,  517, 7484, 3873,\n",
              "            54, 2191, 5330, 3105, 4876, 5330, 1445, 1578, 7095,  807, 7292,\n",
              "          5561,  830, 6828,   54, 2191,  881, 5691, 6909, 2732, 6213, 5859,\n",
              "          2658, 7276, 6516, 7096,  517, 6620, 5495, 7443, 3871, 7075, 2644,\n",
              "          1356, 7670, 5382, 5439, 3860, 5211, 7095, 2135, 6519, 7096, 2959,\n",
              "          6559, 5592, 2562, 6797, 1618, 5561, 5439, 3873,   54,    3]]])"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 토큰 텐서화"
      ],
      "metadata": {
        "id": "q5cavcXZvF3m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_ids1 = torch.tensor(token_ids1).unsqueeze(0)\n",
        "token_ids2 = torch.tensor(token_ids2).unsqueeze(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZQ0QSaUtoEU",
        "outputId": "aab11ef6-7aa9-49bf-9484-d00e4afb9ac1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_ids1[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lB62by2UvcTS",
        "outputId": "6c811ae5-9eb6-4d0d-9a1a-8ecac8054fd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[   2, 2169, 7636, 6559, 2191, 6116,  881, 5691, 6990, 5439,  517, 7815,\n",
              "         2640, 5468, 2191, 5330, 1562, 6113, 7327, 3153, 5439, 2705, 7876, 1185,\n",
              "         5384, 4832, 5414, 7112,   54,  517, 7028, 6559, 7728, 6909, 4720, 7750,\n",
              "         7095, 1535,  833, 2191, 5330, 3873,   54, 5046, 6003, 7568, 6003, 6676,\n",
              "         4888, 6228, 7350,  833,  862, 5827, 5211, 7095, 2135, 5415, 7096, 2822,\n",
              "         7003, 5868, 7096,  517, 7484, 3873,   54, 2191, 5330, 3105, 4876, 5330,\n",
              "         1445, 1578, 7095,  807, 7292, 5561,  830, 6828,   54, 2191,  881, 5691,\n",
              "         6909, 2732, 6213, 5859, 2658, 7276, 6516, 7096,  517, 6620, 5495, 7443,\n",
              "         3871, 7075, 2644, 1356, 7670, 5382, 5439, 3860, 5211, 7095, 2135, 6519,\n",
              "         7096, 2959, 6559, 5592, 2562, 6797, 1618, 5561, 5439, 3873,   54,    3]])"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 문장 표현 추출"
      ],
      "metadata": {
        "id": "tfRLvgMQvIjQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "representation1 = model(token_ids1[0])\n",
        "representation2 = model(token_ids2[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "-SxsqpMUto0M",
        "outputId": "71afc92d-8486-419b-877c-4bddb65b0d83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-66-cd53bbe2d9a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrepresentation1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ids1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mrepresentation2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ids2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    937\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m             \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 939\u001b[0;31m             \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    940\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m             \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cls_rep1 = representation1.last_hidden_state[:, 0, :]\n",
        "cls_rep2 = representation2.last_hidden_state[:, 0, :]"
      ],
      "metadata": {
        "id": "Rm4PaTUttpp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numpy_cls_rep1 = cls_rep1.detach().numpy()\n",
        "numpy_cls_rep2 = cls_rep2.detach().numpy()"
      ],
      "metadata": {
        "id": "-NeGEevNwGtB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "cosine_similarity(numpy_cls_rep1, numpy_cls_rep2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utfuthSfujin",
        "outputId": "a57fe7c6-ed65-4915-c7fb-d7fb8bf87713"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.86600757]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Class화"
      ],
      "metadata": {
        "id": "22KqtYj6yBf5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'\n",
        "!pip install torch==1.9.0 sentencepiece==0.1.96 transformers==4.8.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wn0ox9BZXw1e",
        "outputId": "1f5fa5ea-493d-4dd0-cb41-e6b6fc0bd50b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kobert_tokenizer\n",
            "  Cloning https://github.com/SKTBrain/KoBERT.git to /tmp/pip-install-hnj4dguf/kobert-tokenizer_3c95e3ebc8314454a311fa300bf8e547\n",
            "  Running command git clone -q https://github.com/SKTBrain/KoBERT.git /tmp/pip-install-hnj4dguf/kobert-tokenizer_3c95e3ebc8314454a311fa300bf8e547\n",
            "Requirement already satisfied: torch==1.9.0 in /usr/local/lib/python3.7/dist-packages (1.9.0)\n",
            "Requirement already satisfied: sentencepiece==0.1.96 in /usr/local/lib/python3.7/dist-packages (0.1.96)\n",
            "Requirement already satisfied: transformers==4.8.1 in /usr/local/lib/python3.7/dist-packages (4.8.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0) (3.10.0.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.8.1) (4.10.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.8.1) (1.19.5)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.8.1) (0.10.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.8.1) (21.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers==4.8.1) (3.13)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.8.1) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.8.1) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.8.1) (4.62.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.8.1) (0.0.47)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.8.1) (3.4.2)\n",
            "Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers==4.8.1) (0.0.12)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.8.1) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.8.1) (3.7.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.8.1) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.8.1) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.8.1) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.8.1) (2021.10.8)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.8.1) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.8.1) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.8.1) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertModel\n",
        "from kobert_tokenizer import KoBERTTokenizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "\n",
        "class SentenceSimilarity:\n",
        "  def __init__(self):\n",
        "    self.model = BertModel.from_pretrained('skt/kobert-base-v1')\n",
        "    self.tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
        "\n",
        "  def run(self, sentence1, sentence2):\n",
        "    token_ids1 = self.tokenizer.encode(sentence1)\n",
        "    token_ids2 = self.tokenizer.encode(sentence2)\n",
        "\n",
        "    unsqueeze_token_ids1 = torch.tensor(token_ids1).unsqueeze(0)\n",
        "    unsqueeze_token_ids2 = torch.tensor(token_ids2).unsqueeze(0)\n",
        "\n",
        "    representation1 = self.model(unsqueeze_token_ids1)\n",
        "    representation2 = self.model(unsqueeze_token_ids2)\n",
        "\n",
        "    cls_rep1 = representation1.last_hidden_state[:, 0, :]\n",
        "    cls_rep2 = representation2.last_hidden_state[:, 0, :]\n",
        "\n",
        "    numpy_cls_rep1 = cls_rep1.detach().numpy()\n",
        "    numpy_cls_rep2 = cls_rep2.detach().numpy()\n",
        "\n",
        "    return cosine_similarity(numpy_cls_rep1, numpy_cls_rep2)"
      ],
      "metadata": {
        "id": "oPXnFYNRyBCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_class = SentenceSimilarity()\n",
        "\n",
        "sentence1 = \"고양이가 세상에서 제일 귀엽다.\"\n",
        "sentence2 = \"고양이 보다는 강아지가 더 귀엽지.\"\n",
        "\n",
        "test_class.run(sentence1, sentence2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zbdV3gRRzevL",
        "outputId": "134295b0-0c5d-4abb-e4fa-62ee17696ff9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.81078124]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    }
  ]
}